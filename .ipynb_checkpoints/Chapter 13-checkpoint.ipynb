{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Tensor (object):\n",
    "    \n",
    "    def __init__(self,data,\n",
    "                 autograd=False,\n",
    "                 creators=None,\n",
    "                 creation_op=None,\n",
    "                 id=None):\n",
    "        \n",
    "        self.data = np.array(data)\n",
    "        self.autograd = autograd\n",
    "        self.grad = None\n",
    "        if(id is None):\n",
    "            self.id = np.random.randint(0,100000)\n",
    "        else:\n",
    "            self.id = id\n",
    "        \n",
    "        self.creators = creators\n",
    "        self.creation_op = creation_op\n",
    "        self.children = {}\n",
    "        \n",
    "        if(creators is not None):\n",
    "            for c in creators:\n",
    "                if(self.id not in c.children):\n",
    "                    c.children[self.id] = 1\n",
    "                else:\n",
    "                    c.children[self.id] += 1\n",
    "\n",
    "    def all_children_grads_accounted_for(self):\n",
    "        for id,cnt in self.children.items():\n",
    "            if(cnt != 0):\n",
    "                return False\n",
    "        return True \n",
    "        \n",
    "    def backward(self,grad=None, grad_origin=None):\n",
    "        if(self.autograd):\n",
    " \n",
    "            if(grad is None):\n",
    "                grad = Tensor(np.ones_like(self.data))\n",
    "\n",
    "            if(grad_origin is not None):\n",
    "                if(self.children[grad_origin.id] == 0):\n",
    "                    raise Exception(\"cannot backprop more than once\")\n",
    "                else:\n",
    "                    self.children[grad_origin.id] -= 1\n",
    "\n",
    "            if(self.grad is None):\n",
    "                self.grad = grad\n",
    "            else:\n",
    "                self.grad += grad\n",
    "            \n",
    "            assert grad.autograd == False\n",
    "            \n",
    "            if(self.creators is not None and \n",
    "               (self.all_children_grads_accounted_for() or \n",
    "                grad_origin is None)):\n",
    "\n",
    "                if(self.creation_op == \"add\"):\n",
    "                    self.creators[0].backward(self.grad, self)\n",
    "                    self.creators[1].backward(self.grad, self)\n",
    "                    \n",
    "                if(self.creation_op == \"sub\"):\n",
    "                    self.creators[0].backward(Tensor(self.grad.data), self)\n",
    "                    self.creators[1].backward(Tensor(self.grad.__neg__().data), self)\n",
    "\n",
    "                if(self.creation_op == \"mul\"):\n",
    "                    new = self.grad * self.creators[1]\n",
    "                    self.creators[0].backward(new , self)\n",
    "                    new = self.grad * self.creators[0]\n",
    "                    self.creators[1].backward(new, self)                    \n",
    "                    \n",
    "                if(self.creation_op == \"mm\"):\n",
    "                    c0 = self.creators[0]\n",
    "                    c1 = self.creators[1]\n",
    "                    new = self.grad.mm(c1.transpose())\n",
    "                    c0.backward(new)\n",
    "                    new = self.grad.transpose().mm(c0).transpose()\n",
    "                    c1.backward(new)\n",
    "                    \n",
    "                if(self.creation_op == \"transpose\"):\n",
    "                    self.creators[0].backward(self.grad.transpose())\n",
    "\n",
    "                if(\"sum\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    self.creators[0].backward(self.grad.expand(dim,\n",
    "                                                               self.creators[0].data.shape[dim]))\n",
    "\n",
    "                if(\"expand\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    self.creators[0].backward(self.grad.sum(dim))\n",
    "                    \n",
    "                if(self.creation_op == \"neg\"):\n",
    "                    self.creators[0].backward(self.grad.__neg__())\n",
    "                    \n",
    "                if(self.creation_op == \"sigmoid\"):\n",
    "                    ones = Tensor(np.ones_like(self.grad.data))\n",
    "                    self.creators[0].backward(self.grad * (self * (ones - self)))\n",
    "                \n",
    "                if(self.creation_op == \"tanh\"):\n",
    "                    ones = Tensor(np.ones_like(self.grad.data))\n",
    "                    self.creators[0].backward(self.grad * (ones - (self * self)))\n",
    "                \n",
    "                if(self.creation_op == \"index_select\"):\n",
    "                    new_grad = np.zeros_like(self.creators[0].data)\n",
    "                    indices_ = self.index_select_indices.data.flatten()\n",
    "                    grad_ = grad.data.reshape(len(indices_), -1)\n",
    "                    for i in range(len(indices_)):\n",
    "                        new_grad[indices_[i]] += grad_[i]\n",
    "                    self.creators[0].backward(Tensor(new_grad))\n",
    "                    \n",
    "                if(self.creation_op == \"cross_entropy\"):\n",
    "                    dx = self.softmax_output - self.target_dist\n",
    "                    self.creators[0].backward(Tensor(dx))\n",
    "                    \n",
    "    def __add__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data + other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self,other],\n",
    "                          creation_op=\"add\")\n",
    "        return Tensor(self.data + other.data)\n",
    "\n",
    "    def __neg__(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data * -1,\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"neg\")\n",
    "        return Tensor(self.data * -1)\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data - other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self,other],\n",
    "                          creation_op=\"sub\")\n",
    "        return Tensor(self.data - other.data)\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data * other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self,other],\n",
    "                          creation_op=\"mul\")\n",
    "        return Tensor(self.data * other.data)    \n",
    "\n",
    "    def sum(self, dim):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.sum(dim),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"sum_\"+str(dim))\n",
    "        return Tensor(self.data.sum(dim))\n",
    "    \n",
    "    def expand(self, dim,copies):\n",
    "\n",
    "        trans_cmd = list(range(0,len(self.data.shape)))\n",
    "        trans_cmd.insert(dim,len(self.data.shape))\n",
    "        new_data = self.data.repeat(copies).reshape(list(self.data.shape) + [copies]).transpose(trans_cmd)\n",
    "        \n",
    "        if(self.autograd):\n",
    "            return Tensor(new_data,\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"expand_\"+str(dim))\n",
    "        return Tensor(new_data)\n",
    "    \n",
    "    def transpose(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.transpose(),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"transpose\")\n",
    "        \n",
    "        return Tensor(self.data.transpose())\n",
    "    \n",
    "    def mm(self, x):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.dot(x.data),\n",
    "                          autograd=True,\n",
    "                          creators=[self,x],\n",
    "                          creation_op=\"mm\")\n",
    "        return Tensor(self.data.dot(x.data))\n",
    "    \n",
    "    def sigmoid(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(1 / (1 + np.exp(-self.data)),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"sigmoid\")\n",
    "        return Tensor(1 / (1 + np.exp(-self.data)))\n",
    "\n",
    "    def tanh(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(np.tanh(self.data),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"tanh\")\n",
    "        return Tensor(np.tanh(self.data))\n",
    "    \n",
    "    def index_select(self, indices):\n",
    "        if(self.autograd):\n",
    "            new = Tensor(self.data[indices.data], autograd = True,\n",
    "                        creators = [self],\n",
    "                        creation_op = \"index_select\")\n",
    "            new.index_select_indices = indices\n",
    "            return new\n",
    "        return Tensor(self.data[indices.data])\n",
    "    \n",
    "    def cross_entropy(self, target_indices):\n",
    "\n",
    "        temp = np.exp(self.data)\n",
    "        softmax_output = temp / np.sum(temp,\n",
    "                                       axis=len(self.data.shape)-1,\n",
    "                                       keepdims=True)\n",
    "        \n",
    "        t = target_indices.data.flatten()\n",
    "        p = softmax_output.reshape(len(t),-1)\n",
    "        target_dist = np.eye(p.shape[1])[t]\n",
    "        loss = -(np.log(p) * (target_dist)).sum(1).mean()\n",
    "    \n",
    "        if(self.autograd):\n",
    "            out = Tensor(loss,\n",
    "                         autograd=True,\n",
    "                         creators=[self],\n",
    "                         creation_op=\"cross_entropy\")\n",
    "            out.softmax_output = softmax_output\n",
    "            out.target_dist = target_dist\n",
    "            return out\n",
    "        return Tensor(loss)\n",
    "        \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())  \n",
    "    \n",
    "class SGD(object):\n",
    "    def __init__(self, parameters, alpha = 0.1):\n",
    "        self.parameters = parameters\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def zero(self):\n",
    "        for p in self.parameters:\n",
    "            p.grad.data *= 0\n",
    "            \n",
    "    def step(self, zero = True):\n",
    "        for p in self.parameters:\n",
    "            p.data -= p.grad.data * self.alpha\n",
    "            if(zero):\n",
    "                p.grad.data *= 0\n",
    "                \n",
    "class Layer(object):\n",
    "    def __init__(self):\n",
    "        self.parameters = list()\n",
    "        \n",
    "    def get_parameters(self):\n",
    "        return self.parameters\n",
    "    \n",
    "class Linear(Layer):\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        super().__init__()\n",
    "        w = np.random.randn(n_inputs, n_outputs) * np.sqrt(2/(n_inputs))\n",
    "        self.weight = Tensor(w, autograd = True)\n",
    "        self.bias = Tensor(np.zeros(n_outputs), autograd = True)    \n",
    "        self.parameters.append(self.weight)\n",
    "        self.parameters.append(self.bias)\n",
    "            \n",
    "    def forward(self, input):\n",
    "        return input.mm(self.weight) + self.bias.expand(0, len(input.data))\n",
    "    \n",
    "class Sequential(Layer):\n",
    "    def __init__(self, layers = list()):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "            \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "            \n",
    "    def forward(self, input):\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "        return input\n",
    "        \n",
    "    def get_parameters(self):\n",
    "        params = list()\n",
    "        for l in self.layers:\n",
    "            params += l.get_parameters()\n",
    "        return params\n",
    "        \n",
    "class MSELoss(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        return ((pred - target)*(pred - target)).sum(0)\n",
    "    \n",
    "class Tanh(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return input.tanh()\n",
    "    \n",
    "class Sigmoid(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return input.sigmoid()\n",
    "    \n",
    "class Embedding(Layer):\n",
    "    def __init__(self, vocab_size, dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.dim = dim\n",
    "        \n",
    "        weight = (np.random.rand(vocab_size, dim) - 0.5) / dim\n",
    "        self.weight = Tensor(weight, autograd = True)\n",
    "        \n",
    "        self.parameters.append(self.weight)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.weight.index_select(input)\n",
    "    \n",
    "class CrossEntropyLoss(object):\n",
    "    def __init__sef():\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        return input.cross_entropy(target)\n",
    "    \n",
    "class RNNCell(Layer):\n",
    "    def __init__(self, n_inputs, n_hidden, n_output, activation = \"sigmoid\"):\n",
    "        super().__init__()\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        \n",
    "        if(activation == \"sigmoid\"):\n",
    "            self.activation = Sigmoid()\n",
    "        elif(activation == \"tanh\"):\n",
    "            self.activation = Tanh()\n",
    "        else:\n",
    "            raise Exception(\"Non-linearity not found\")\n",
    "            \n",
    "        self.w_ih = Linear(n_inputs, n_hidden)\n",
    "        self.w_hh = Linear(n_hidden, n_hidden)\n",
    "        self.w_ho = Linear(n_hidden, n_output)\n",
    "        \n",
    "        self.parameters += self.w_ih.get_parameters()\n",
    "        self.parameters += self.w_hh.get_parameters()\n",
    "        self.parameters += self.w_ho.get_parameters()\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        from_prev_hidden = self.w_hh.forward(hidden)\n",
    "        combined = self.w_ih.forward(input) + from_prev_hidden\n",
    "        new_hidden = self.activation.forward(combined)\n",
    "        output = self.w_ho.forward(new_hidden)\n",
    "        return output, new_hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size = 1):\n",
    "        return Tensor(np.zeros((batch_size, self.n_hidden)), autograd =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1]\n",
      "[-2 -2 -2 -2 -2]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n",
      "[1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "a = Tensor([1,2,3,4,5], autograd = True)\n",
    "b = Tensor([2,2,2,2,2], autograd = True)\n",
    "c = Tensor([5,4,3,2,1], autograd = True)\n",
    "\n",
    "d = a + (-b)\n",
    "e = (-b) + c\n",
    "f = d + e\n",
    "\n",
    "f.backward(Tensor(np.array([1,1,1,1,1])))\n",
    "\n",
    "print(a.grad.data)\n",
    "print(b.grad.data)\n",
    "print(c.grad.data)\n",
    "print(d.grad.data)\n",
    "print(e.grad.data)\n",
    "print(f.grad.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.12427324]\n",
      "[0.64112616]\n",
      "[0.44318917]\n",
      "[0.31387083]\n",
      "[0.20897697]\n",
      "[0.12969368]\n",
      "[0.07493533]\n",
      "[0.04045744]\n",
      "[0.02057729]\n",
      "[0.00996295]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "data = Tensor(np.array([[0,0],[0,1],[1,0],[1,1]]), autograd=True)\n",
    "target = Tensor(np.array([[0],[1],[0],[1]]), autograd=True)\n",
    "\n",
    "weights = list()\n",
    "weights.append(Tensor(np.random.rand(2,3), autograd=True))\n",
    "weights.append(Tensor(np.random.rand(3,1), autograd=True))\n",
    "\n",
    "for i in range(10):\n",
    "    pred = data.mm(weights[0]).mm(weights[1])\n",
    "    loss = ((pred - target)*(pred - target)).sum(0)\n",
    "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
    "    \n",
    "    for w in weights:\n",
    "        w.data -= w.grad.data * 0.1\n",
    "        w.grad.data *= 0\n",
    "\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.12427324]\n",
      "[0.64112616]\n",
      "[0.44318917]\n",
      "[0.31387083]\n",
      "[0.20897697]\n",
      "[0.12969368]\n",
      "[0.07493533]\n",
      "[0.04045744]\n",
      "[0.02057729]\n",
      "[0.00996295]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "data = Tensor(np.array([[0,0],[0,1],[1,0],[1,1]]), autograd=True)\n",
    "target = Tensor(np.array([[0],[1],[0],[1]]), autograd=True)\n",
    "\n",
    "weights = list()\n",
    "weights.append(Tensor(np.random.rand(2,3), autograd=True))\n",
    "weights.append(Tensor(np.random.rand(3,1), autograd=True))\n",
    "\n",
    "optim = SGD(parameters=weights, alpha=0.1)\n",
    "\n",
    "for i in range(10):\n",
    "    pred = data.mm(weights[0]).mm(weights[1])\n",
    "    loss = ((pred - target)*(pred - target)).sum(0)\n",
    "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
    "    optim.step()\n",
    "\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.58851814]\n",
      "[0.53258841]\n",
      "[4.65643674]\n",
      "[35.63491234]\n",
      "[36.18671332]\n",
      "[99.85775706]\n",
      "[39.46443576]\n",
      "[10060.88863906]\n",
      "[2.96794571e+09]\n",
      "[1.06242054e+26]\n"
     ]
    }
   ],
   "source": [
    "data = Tensor(np.array([[0,0], [0,1], [1,0], [1,1]]), autograd = True)\n",
    "target = Tensor(np.array([[0], [1], [0], [1]]), autograd = True)\n",
    "\n",
    "model = Sequential([Linear(2,3), Linear(3,1)])\n",
    "optim = SGD(parameters = model.get_parameters(), alpha = 0.05)\n",
    "\n",
    "for i in range(10):\n",
    "    pred = model.forward(data)\n",
    "    loss = ((pred - target) * (pred - target)).sum(0)\n",
    "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
    "    optim.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.32867133]\n",
      "[0.38326249]\n",
      "[0.17697789]\n",
      "[0.12255879]\n",
      "[0.09078094]\n",
      "[0.06911117]\n",
      "[0.05373838]\n",
      "[0.04245233]\n",
      "[0.03390907]\n",
      "[0.02728052]\n"
     ]
    }
   ],
   "source": [
    "data = Tensor(np.array([[0,0], [0,1], [1,0], [1,1]]), autograd = True)\n",
    "target = Tensor(np.array([[0], [1], [0], [1]]), autograd = True)\n",
    "\n",
    "model = Sequential([Linear(2,3), Linear(3,1)])\n",
    "optim = SGD(parameters = model.get_parameters(), alpha = 0.05)\n",
    "criterion = MSELoss()\n",
    "\n",
    "for i in range(10):\n",
    "    pred = model.forward(data)\n",
    "    loss = criterion.forward(pred, target)\n",
    "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
    "    optim.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.18121562]\n",
      "[0.77282441]\n",
      "[0.54610058]\n",
      "[0.35495787]\n",
      "[0.23000602]\n",
      "[0.15698963]\n",
      "[0.11390411]\n",
      "[0.08712864]\n",
      "[0.06943514]\n",
      "[0.0570973]\n"
     ]
    }
   ],
   "source": [
    "data = Tensor(np.array([[0,0],[0,1],[1,0],[1,1]]), autograd=True)\n",
    "target = Tensor(np.array([[0],[1],[0],[1]]), autograd=True)\n",
    "\n",
    "model = Sequential([Linear(2,3), Tanh(), Linear(3,1), Sigmoid()])\n",
    "criterion = MSELoss()\n",
    "\n",
    "optim = SGD(parameters=model.get_parameters(), alpha=1)\n",
    "\n",
    "for i in range(10):\n",
    "    pred = model.forward(data)\n",
    "    loss = criterion.forward(pred, target)\n",
    "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
    "    optim.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.03597284]\n",
      "[0.6053573]\n",
      "[0.34029019]\n",
      "[0.18851853]\n",
      "[0.11855053]\n",
      "[0.08308112]\n",
      "[0.06262093]\n",
      "[0.04960446]\n",
      "[0.0407139]\n",
      "[0.03431255]\n"
     ]
    }
   ],
   "source": [
    "data = Tensor(np.array([1, 2, 1, 2]), autograd=True)\n",
    "target = Tensor(np.array([[0],[1],[0],[1]]), autograd=True)\n",
    "\n",
    "model = Sequential([Embedding(5,3), Tanh(), Linear(3,1), Sigmoid()])\n",
    "criterion = MSELoss()\n",
    "\n",
    "optim = SGD(parameters=model.get_parameters(), alpha=1)\n",
    "\n",
    "for i in range(10):\n",
    "    pred = model.forward(data)\n",
    "    loss = criterion.forward(pred, target)\n",
    "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
    "    optim.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2772799770415606\n",
      "0.9316691523502416\n",
      "0.6991689385224904\n",
      "0.5393340453712206\n",
      "0.42686450937652054\n",
      "0.34601834775353\n",
      "0.28665853773543665\n",
      "0.24212316273825107\n",
      "0.2079896503488622\n",
      "0.18129261801406055\n"
     ]
    }
   ],
   "source": [
    "data = Tensor(np.array([1,2,1,2]), autograd=True)\n",
    "target = Tensor(np.array([0,1,0,1]), autograd=True)\n",
    "\n",
    "model = Sequential([Embedding(3,3), Tanh(), Linear(3,4)])\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "optim = SGD(parameters=model.get_parameters(), alpha=0.1)\n",
    "\n",
    "for i in range(10):\n",
    "    pred = model.forward(data)\n",
    "    loss = criterion.forward(pred, target)\n",
    "    loss.backward(Tensor(np.ones_like(loss.data)))\n",
    "    optim.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "f = open('qa1_single-supporting-fact_train.txt', 'r')\n",
    "raw = f.readlines()\n",
    "f.close()\n",
    "\n",
    "tokens = list()\n",
    "for line in raw[0:1000]:\n",
    "    tokens.append(line.lower().replace(\"\\n\", \"\").split(\" \")[1:])\n",
    "    \n",
    "new_tokens = list()\n",
    "for line in tokens:\n",
    "    new_tokens.append(['-'] * (6 - len(line)) + line)\n",
    "tokens = new_tokens\n",
    "\n",
    "vocab = set()\n",
    "for sent in tokens:\n",
    "    for word in sent:\n",
    "        vocab.add(word)\n",
    "vocab = list(vocab)\n",
    "\n",
    "word2index = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    word2index[word] = i\n",
    "    \n",
    "def words2indices(sentence):\n",
    "    idx = list()\n",
    "    for word in sentence:\n",
    "        idx.append(word2index[word])\n",
    "    return idx\n",
    "\n",
    "indices = list()\n",
    "for line in tokens:\n",
    "    idx = list()\n",
    "    for w in line:\n",
    "        idx.append(word2index[w])\n",
    "    indices.append(idx)\n",
    "    \n",
    "data = np.array(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.49188299424959636 % Correct: 0.0\n",
      "Loss: 0.33242056426645106 % Correct: 0.14\n",
      "Loss: 0.3168957052040789 % Correct: 0.13\n",
      "Loss: 0.2987106296326819 % Correct: 0.14\n",
      "Loss: 0.3124638876950327 % Correct: 0.13\n",
      "Loss: 0.30309274689537696 % Correct: 0.14\n",
      "Loss: 0.29548654097474825 % Correct: 0.13\n",
      "Loss: 0.2700991418503088 % Correct: 0.14\n",
      "Loss: 0.28209515745283287 % Correct: 0.13\n",
      "Loss: 0.28740359686218253 % Correct: 0.18\n",
      "Loss: 0.28193003962166846 % Correct: 0.18\n",
      "Loss: 0.24595148124373106 % Correct: 0.2\n",
      "Loss: 0.24739037248928444 % Correct: 0.18\n",
      "Loss: 0.281169684691183 % Correct: 0.2\n",
      "Loss: 0.2754616957672565 % Correct: 0.19\n",
      "Loss: 0.26780440781830855 % Correct: 0.18\n",
      "Loss: 0.2421571388141658 % Correct: 0.2\n",
      "Loss: 0.2493762035107659 % Correct: 0.19\n",
      "Loss: 0.25108643819860643 % Correct: 0.2\n",
      "Loss: 0.2425524553278145 % Correct: 0.18\n",
      "Loss: 0.22606436902978352 % Correct: 0.2\n",
      "Loss: 0.22700453659789085 % Correct: 0.19\n",
      "Loss: 0.23586348760497677 % Correct: 0.2\n",
      "Loss: 0.2508643546049519 % Correct: 0.19\n",
      "Loss: 0.23179340969906845 % Correct: 0.17\n",
      "Loss: 0.21613585825884055 % Correct: 0.18\n",
      "Loss: 0.21743210775774968 % Correct: 0.17\n",
      "Loss: 0.22615499345546186 % Correct: 0.19\n",
      "Loss: 0.23379266903725693 % Correct: 0.2\n",
      "Loss: 0.23778634227891274 % Correct: 0.2\n",
      "Loss: 0.2159429109679168 % Correct: 0.18\n",
      "Loss: 0.2082610380257389 % Correct: 0.2\n",
      "Loss: 0.2093098716667205 % Correct: 0.19\n",
      "Loss: 0.21477004741398686 % Correct: 0.2\n",
      "Loss: 0.22112523971213008 % Correct: 0.19\n",
      "Loss: 0.2174652112582164 % Correct: 0.21\n",
      "Loss: 0.20596170145962406 % Correct: 0.18\n",
      "Loss: 0.20368257470100676 % Correct: 0.22\n",
      "Loss: 0.2031619274728232 % Correct: 0.2\n",
      "Loss: 0.20687686475431555 % Correct: 0.22\n",
      "Loss: 0.2086699000337609 % Correct: 0.2\n",
      "Loss: 0.20692872331105044 % Correct: 0.22\n",
      "Loss: 0.20566095108183716 % Correct: 0.2\n",
      "Loss: 0.20056198751788093 % Correct: 0.22\n",
      "Loss: 0.20036744198022421 % Correct: 0.2\n",
      "Loss: 0.1990497922647435 % Correct: 0.22\n",
      "Loss: 0.20043121620678836 % Correct: 0.2\n",
      "Loss: 0.19812014489786417 % Correct: 0.22\n",
      "Loss: 0.19866795554055341 % Correct: 0.2\n",
      "Loss: 0.19579507575220562 % Correct: 0.22\n",
      "Loss: 0.19635746435299756 % Correct: 0.2\n",
      "Loss: 0.19434517720790584 % Correct: 0.22\n",
      "Loss: 0.19535074663556923 % Correct: 0.2\n",
      "Loss: 0.19339104407485747 % Correct: 0.22\n",
      "Loss: 0.1943222553888805 % Correct: 0.2\n",
      "Loss: 0.19224204698663852 % Correct: 0.22\n",
      "Loss: 0.19307059473789953 % Correct: 0.2\n",
      "Loss: 0.19119790699764894 % Correct: 0.22\n",
      "Loss: 0.1921096759175345 % Correct: 0.2\n",
      "Loss: 0.19037572510875303 % Correct: 0.22\n",
      "Loss: 0.1913309373163454 % Correct: 0.2\n",
      "Loss: 0.1896100980492686 % Correct: 0.22\n",
      "Loss: 0.1905567656186919 % Correct: 0.2\n",
      "Loss: 0.18887658872947294 % Correct: 0.22\n",
      "Loss: 0.1898451464273049 % Correct: 0.2\n",
      "Loss: 0.1882215536273954 % Correct: 0.22\n",
      "Loss: 0.18923503681682802 % Correct: 0.2\n",
      "Loss: 0.1876318749210269 % Correct: 0.22\n",
      "Loss: 0.18868563075728234 % Correct: 0.2\n",
      "Loss: 0.1870813743918493 % Correct: 0.22\n",
      "Loss: 0.18817679139388216 % Correct: 0.2\n",
      "Loss: 0.18656786285986218 % Correct: 0.22\n",
      "Loss: 0.1877168018916442 % Correct: 0.2\n",
      "Loss: 0.18609220742184077 % Correct: 0.22\n",
      "Loss: 0.18730527591025672 % Correct: 0.2\n",
      "Loss: 0.18564766689496412 % Correct: 0.22\n",
      "Loss: 0.1869337800117903 % Correct: 0.2\n",
      "Loss: 0.18522808175529487 % Correct: 0.22\n",
      "Loss: 0.18659931925907094 % Correct: 0.2\n",
      "Loss: 0.1848312498518968 % Correct: 0.22\n",
      "Loss: 0.1863035988410094 % Correct: 0.2\n",
      "Loss: 0.18445692628891724 % Correct: 0.22\n",
      "Loss: 0.18604858666909302 % Correct: 0.2\n",
      "Loss: 0.18410816853098172 % Correct: 0.22\n",
      "Loss: 0.1858373948660735 % Correct: 0.2\n",
      "Loss: 0.1837969000760108 % Correct: 0.22\n",
      "Loss: 0.18567533072585812 % Correct: 0.2\n",
      "Loss: 0.183551766708516 % Correct: 0.22\n",
      "Loss: 0.18556458447592672 % Correct: 0.2\n",
      "Loss: 0.1834240344167405 % Correct: 0.22\n",
      "Loss: 0.1854870584458992 % Correct: 0.21\n",
      "Loss: 0.18346810783597847 % Correct: 0.22\n",
      "Loss: 0.18537694777468686 % Correct: 0.21\n",
      "Loss: 0.18365453443721105 % Correct: 0.22\n",
      "Loss: 0.18513434576092117 % Correct: 0.21\n",
      "Loss: 0.18379140318514298 % Correct: 0.22\n",
      "Loss: 0.18474020704066843 % Correct: 0.21\n",
      "Loss: 0.18371094709914168 % Correct: 0.22\n",
      "Loss: 0.1843084812361319 % Correct: 0.21\n",
      "Loss: 0.18348440950921252 % Correct: 0.22\n",
      "Loss: 0.1839546500466011 % Correct: 0.21\n",
      "Loss: 0.1832417576562007 % Correct: 0.22\n",
      "Loss: 0.18370218927391813 % Correct: 0.21\n",
      "Loss: 0.1830220788880999 % Correct: 0.22\n",
      "Loss: 0.18351392142711134 % Correct: 0.21\n",
      "Loss: 0.18281535437583799 % Correct: 0.22\n",
      "Loss: 0.1833519086559144 % Correct: 0.21\n",
      "Loss: 0.1826137968518683 % Correct: 0.22\n",
      "Loss: 0.18320202093754256 % Correct: 0.21\n",
      "Loss: 0.18242068886926627 % Correct: 0.22\n",
      "Loss: 0.18306443176223255 % Correct: 0.21\n",
      "Loss: 0.18224026998779042 % Correct: 0.22\n",
      "Loss: 0.1829387880751296 % Correct: 0.21\n",
      "Loss: 0.18207269776370683 % Correct: 0.22\n",
      "Loss: 0.18282128812765147 % Correct: 0.21\n",
      "Loss: 0.18191640965997471 % Correct: 0.22\n",
      "Loss: 0.18270844531363606 % Correct: 0.21\n",
      "Loss: 0.18177067544583875 % Correct: 0.22\n",
      "Loss: 0.1825987287380842 % Correct: 0.21\n",
      "Loss: 0.18163551538454922 % Correct: 0.22\n",
      "Loss: 0.1824912163949493 % Correct: 0.21\n",
      "Loss: 0.18151067131997367 % Correct: 0.22\n",
      "Loss: 0.1823842905700746 % Correct: 0.21\n",
      "Loss: 0.18139512112251788 % Correct: 0.22\n",
      "Loss: 0.1822755895299369 % Correct: 0.21\n",
      "Loss: 0.1812870638879825 % Correct: 0.22\n",
      "Loss: 0.1821625134966087 % Correct: 0.21\n",
      "Loss: 0.18118399348978328 % Correct: 0.22\n",
      "Loss: 0.18204270022621397 % Correct: 0.21\n",
      "Loss: 0.18108291361530798 % Correct: 0.22\n",
      "Loss: 0.18191448935385607 % Correct: 0.21\n",
      "Loss: 0.18098085519294144 % Correct: 0.22\n",
      "Loss: 0.1817773852156304 % Correct: 0.21\n",
      "Loss: 0.18087554696033864 % Correct: 0.22\n",
      "Loss: 0.18163225276898723 % Correct: 0.21\n",
      "Loss: 0.1807658424818004 % Correct: 0.22\n",
      "Loss: 0.1814810058812511 % Correct: 0.21\n",
      "Loss: 0.18065163694414949 % Correct: 0.22\n",
      "Loss: 0.18132588761704255 % Correct: 0.21\n",
      "Loss: 0.18053337982481268 % Correct: 0.22\n",
      "Loss: 0.18116874052365392 % Correct: 0.21\n",
      "Loss: 0.18041153983759173 % Correct: 0.22\n",
      "Loss: 0.18101067836842233 % Correct: 0.21\n",
      "Loss: 0.18028634727094497 % Correct: 0.22\n",
      "Loss: 0.18085243689925443 % Correct: 0.21\n",
      "Loss: 0.180158019872601 % Correct: 0.22\n",
      "Loss: 0.18069586877703198 % Correct: 0.21\n",
      "Loss: 0.1800278514001294 % Correct: 0.22\n",
      "Loss: 0.18054857214066464 % Correct: 0.21\n",
      "Loss: 0.1799015951299286 % Correct: 0.22\n",
      "Loss: 0.18043987277646004 % Correct: 0.21\n",
      "Loss: 0.1797988859834909 % Correct: 0.22\n",
      "Loss: 0.18047608688186498 % Correct: 0.21\n",
      "Loss: 0.17976033586793663 % Correct: 0.27\n",
      "Loss: 0.1809534931461078 % Correct: 0.21\n",
      "Loss: 0.17969736523293814 % Correct: 0.23\n",
      "Loss: 0.1819395621543757 % Correct: 0.21\n",
      "Loss: 0.1792262548562199 % Correct: 0.19\n",
      "Loss: 0.18204645264744995 % Correct: 0.22\n",
      "Loss: 0.1794940907570153 % Correct: 0.19\n",
      "Loss: 0.1832462437678306 % Correct: 0.22\n",
      "Loss: 0.18113711520757111 % Correct: 0.19\n",
      "Loss: 0.1837081977388313 % Correct: 0.22\n",
      "Loss: 0.18223514112791428 % Correct: 0.26\n",
      "Loss: 0.18217482801956672 % Correct: 0.22\n",
      "Loss: 0.180868322106628 % Correct: 0.23\n",
      "Loss: 0.18106147028210393 % Correct: 0.22\n",
      "Loss: 0.18009853660292224 % Correct: 0.2\n",
      "Loss: 0.18058689331526706 % Correct: 0.22\n",
      "Loss: 0.17968383047508077 % Correct: 0.19\n",
      "Loss: 0.18056556288175185 % Correct: 0.22\n",
      "Loss: 0.17943275707801767 % Correct: 0.19\n",
      "Loss: 0.18070795586482843 % Correct: 0.22\n",
      "Loss: 0.17920871418761236 % Correct: 0.19\n",
      "Loss: 0.1808433077582164 % Correct: 0.22\n",
      "Loss: 0.1789929151831376 % Correct: 0.19\n",
      "Loss: 0.18096448495774425 % Correct: 0.22\n",
      "Loss: 0.1788002076784558 % Correct: 0.19\n",
      "Loss: 0.18108212293153583 % Correct: 0.22\n",
      "Loss: 0.17863506564425696 % Correct: 0.19\n",
      "Loss: 0.1811705062581209 % Correct: 0.22\n",
      "Loss: 0.17849508240356365 % Correct: 0.19\n",
      "Loss: 0.18121554435627713 % Correct: 0.22\n",
      "Loss: 0.17836605348818263 % Correct: 0.19\n",
      "Loss: 0.18120962729291543 % Correct: 0.21\n",
      "Loss: 0.17823314507531265 % Correct: 0.19\n",
      "Loss: 0.181165303237478 % Correct: 0.22\n",
      "Loss: 0.17808643062423388 % Correct: 0.19\n",
      "Loss: 0.1810978631688535 % Correct: 0.22\n",
      "Loss: 0.17792696630200283 % Correct: 0.22\n",
      "Loss: 0.1810257579578553 % Correct: 0.22\n",
      "Loss: 0.17776146946984528 % Correct: 0.22\n",
      "Loss: 0.18095657884540067 % Correct: 0.22\n",
      "Loss: 0.1776012372780864 % Correct: 0.22\n",
      "Loss: 0.1808963342931628 % Correct: 0.22\n",
      "Loss: 0.17745514179193905 % Correct: 0.22\n",
      "Loss: 0.18083914929140163 % Correct: 0.22\n",
      "Loss: 0.17732996588253222 % Correct: 0.24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.18078034776902394 % Correct: 0.22\n",
      "Loss: 0.17722383539051112 % Correct: 0.23\n",
      "Loss: 0.18070555088661308 % Correct: 0.22\n",
      "Loss: 0.177131008190999 % Correct: 0.23\n",
      "Loss: 0.18060996574792537 % Correct: 0.22\n",
      "Loss: 0.1770398019625641 % Correct: 0.23\n",
      "Loss: 0.18048413642947816 % Correct: 0.22\n",
      "Loss: 0.17694346889983276 % Correct: 0.23\n",
      "Loss: 0.18033377347223958 % Correct: 0.22\n",
      "Loss: 0.1768361394516175 % Correct: 0.23\n",
      "Loss: 0.18015643267424503 % Correct: 0.22\n",
      "Loss: 0.17671959233575732 % Correct: 0.23\n",
      "Loss: 0.1799615371289767 % Correct: 0.22\n",
      "Loss: 0.17659368752999466 % Correct: 0.23\n",
      "Loss: 0.17974638639749912 % Correct: 0.22\n",
      "Loss: 0.17646205435655876 % Correct: 0.23\n",
      "Loss: 0.1795181786363616 % Correct: 0.22\n",
      "Loss: 0.17632471923615373 % Correct: 0.23\n",
      "Loss: 0.17927466555448116 % Correct: 0.22\n",
      "Loss: 0.17618405982437052 % Correct: 0.23\n",
      "Loss: 0.17902211423905398 % Correct: 0.22\n",
      "Loss: 0.1760402445515503 % Correct: 0.23\n",
      "Loss: 0.17876182766401003 % Correct: 0.22\n",
      "Loss: 0.17589492272044077 % Correct: 0.23\n",
      "Loss: 0.17850024474365822 % Correct: 0.22\n",
      "Loss: 0.17574874705855248 % Correct: 0.23\n",
      "Loss: 0.17824130374630473 % Correct: 0.22\n",
      "Loss: 0.17560283594907272 % Correct: 0.23\n",
      "Loss: 0.17799009592877196 % Correct: 0.22\n",
      "Loss: 0.1754578150318612 % Correct: 0.24\n",
      "Loss: 0.17774979035885421 % Correct: 0.22\n",
      "Loss: 0.17531419298912432 % Correct: 0.24\n",
      "Loss: 0.17752261403449304 % Correct: 0.22\n",
      "Loss: 0.1751721476043615 % Correct: 0.21\n",
      "Loss: 0.17730934519791608 % Correct: 0.22\n",
      "Loss: 0.17503163194746707 % Correct: 0.21\n",
      "Loss: 0.17710981606020976 % Correct: 0.22\n",
      "Loss: 0.1748924198704727 % Correct: 0.25\n",
      "Loss: 0.1769232034588326 % Correct: 0.22\n",
      "Loss: 0.17475420177033896 % Correct: 0.25\n",
      "Loss: 0.1767483851159216 % Correct: 0.22\n",
      "Loss: 0.17461666531477715 % Correct: 0.24\n",
      "Loss: 0.17658418089252761 % Correct: 0.25\n",
      "Loss: 0.17447955697828987 % Correct: 0.26\n",
      "Loss: 0.1764295104543651 % Correct: 0.25\n",
      "Loss: 0.17434271804372745 % Correct: 0.25\n",
      "Loss: 0.17628348651258627 % Correct: 0.25\n",
      "Loss: 0.1742060986011657 % Correct: 0.25\n",
      "Loss: 0.17614547496060254 % Correct: 0.25\n",
      "Loss: 0.17406975444038722 % Correct: 0.25\n",
      "Loss: 0.17601513981799716 % Correct: 0.24\n",
      "Loss: 0.17393383137875001 % Correct: 0.26\n",
      "Loss: 0.1758924771061911 % Correct: 0.24\n",
      "Loss: 0.17379854136041706 % Correct: 0.26\n",
      "Loss: 0.17577783284085166 % Correct: 0.24\n",
      "Loss: 0.17366413521149407 % Correct: 0.27\n",
      "Loss: 0.17567189822508797 % Correct: 0.24\n",
      "Loss: 0.17353087780134774 % Correct: 0.27\n",
      "Loss: 0.17557567809678137 % Correct: 0.24\n",
      "Loss: 0.17339903343769272 % Correct: 0.27\n",
      "Loss: 0.17549043519706858 % Correct: 0.24\n",
      "Loss: 0.1732688762866233 % Correct: 0.28\n",
      "Loss: 0.17541762384824702 % Correct: 0.24\n",
      "Loss: 0.17314075914759794 % Correct: 0.28\n",
      "Loss: 0.17535884443566346 % Correct: 0.24\n",
      "Loss: 0.17301531458805797 % Correct: 0.28\n",
      "Loss: 0.17531587545823996 % Correct: 0.23\n",
      "Loss: 0.1728939450454472 % Correct: 0.28\n",
      "Loss: 0.1752908740971687 % Correct: 0.25\n",
      "Loss: 0.17277992574066764 % Correct: 0.26\n",
      "Loss: 0.17528689291680302 % Correct: 0.25\n",
      "Loss: 0.1726807753152143 % Correct: 0.26\n",
      "Loss: 0.17530897900684692 % Correct: 0.25\n",
      "Loss: 0.17261308969673278 % Correct: 0.26\n",
      "Loss: 0.1753663236114072 % Correct: 0.23\n",
      "Loss: 0.1726111953226862 % Correct: 0.27\n",
      "Loss: 0.17547580728527234 % Correct: 0.21\n",
      "Loss: 0.17273669413728454 % Correct: 0.27\n",
      "Loss: 0.17566395537978458 % Correct: 0.21\n",
      "Loss: 0.17306285131909505 % Correct: 0.28\n",
      "Loss: 0.17594749068285268 % Correct: 0.21\n",
      "Loss: 0.17356055351478952 % Correct: 0.28\n",
      "Loss: 0.17624359182792443 % Correct: 0.21\n",
      "Loss: 0.1739195404940273 % Correct: 0.28\n",
      "Loss: 0.17627617396433126 % Correct: 0.21\n",
      "Loss: 0.17379271644732638 % Correct: 0.28\n",
      "Loss: 0.1758794063562018 % Correct: 0.21\n",
      "Loss: 0.17332520214123134 % Correct: 0.28\n",
      "Loss: 0.17536570031894722 % Correct: 0.21\n",
      "Loss: 0.1728441720973383 % Correct: 0.28\n",
      "Loss: 0.17505249275043505 % Correct: 0.21\n",
      "Loss: 0.17245822711255807 % Correct: 0.27\n",
      "Loss: 0.17493309782283678 % Correct: 0.21\n",
      "Loss: 0.17214425139412573 % Correct: 0.27\n",
      "Loss: 0.17488145461741128 % Correct: 0.21\n",
      "Loss: 0.17187773135319934 % Correct: 0.27\n",
      "Loss: 0.17483516663725565 % Correct: 0.21\n",
      "Loss: 0.17165184440081532 % Correct: 0.27\n",
      "Loss: 0.17478469790906073 % Correct: 0.21\n",
      "Loss: 0.17146248356918428 % Correct: 0.27\n",
      "Loss: 0.17472441866944224 % Correct: 0.23\n",
      "Loss: 0.1713080842142986 % Correct: 0.27\n",
      "Loss: 0.174650875791318 % Correct: 0.23\n",
      "Loss: 0.1711919208280518 % Correct: 0.26\n",
      "Loss: 0.1745623833942495 % Correct: 0.23\n",
      "Loss: 0.1711252753379392 % Correct: 0.28\n",
      "Loss: 0.17445543220226764 % Correct: 0.24\n",
      "Loss: 0.17113316394404893 % Correct: 0.28\n",
      "Loss: 0.1743189518609674 % Correct: 0.26\n",
      "Loss: 0.1712592882832244 % Correct: 0.26\n",
      "Loss: 0.17412409177474578 % Correct: 0.26\n",
      "Loss: 0.17155135363916454 % Correct: 0.26\n",
      "Loss: 0.17381931805327425 % Correct: 0.24\n",
      "Loss: 0.17199030726120207 % Correct: 0.26\n",
      "Loss: 0.17338516533641266 % Correct: 0.23\n",
      "Loss: 0.17244252684974298 % Correct: 0.26\n",
      "Loss: 0.17297032184260394 % Correct: 0.23\n",
      "Loss: 0.1729370024938289 % Correct: 0.28\n",
      "Loss: 0.17273009377471146 % Correct: 0.23\n",
      "Loss: 0.1736900528517488 % Correct: 0.28\n",
      "Loss: 0.1722297134236987 % Correct: 0.25\n",
      "Loss: 0.1738861749811086 % Correct: 0.28\n",
      "Loss: 0.1711294312633821 % Correct: 0.24\n",
      "Loss: 0.17268270514825892 % Correct: 0.28\n",
      "Loss: 0.17016328301511593 % Correct: 0.24\n",
      "Loss: 0.17140579502097036 % Correct: 0.28\n",
      "Loss: 0.1696379013760355 % Correct: 0.24\n",
      "Loss: 0.17069446027214397 % Correct: 0.28\n",
      "Loss: 0.16950311237733506 % Correct: 0.24\n",
      "Loss: 0.1703903770695613 % Correct: 0.28\n",
      "Loss: 0.16989176411574625 % Correct: 0.24\n",
      "Loss: 0.17038484499217568 % Correct: 0.27\n",
      "Loss: 0.17142212804950915 % Correct: 0.25\n",
      "Loss: 0.17046458381911334 % Correct: 0.26\n",
      "Loss: 0.17414174990454184 % Correct: 0.23\n",
      "Loss: 0.16990349743221123 % Correct: 0.26\n",
      "Loss: 0.17509263328342434 % Correct: 0.24\n",
      "Loss: 0.1707944072426315 % Correct: 0.27\n",
      "Loss: 0.17718013513770203 % Correct: 0.23\n",
      "Loss: 0.17373789114075405 % Correct: 0.25\n",
      "Loss: 0.1785163922013814 % Correct: 0.23\n",
      "Loss: 0.17290891318848686 % Correct: 0.26\n",
      "Loss: 0.17325239652890917 % Correct: 0.28\n",
      "Loss: 0.1702041780132098 % Correct: 0.27\n",
      "Loss: 0.17039591894047285 % Correct: 0.27\n",
      "Loss: 0.16934768202350545 % Correct: 0.26\n",
      "Loss: 0.1698419256283778 % Correct: 0.27\n",
      "Loss: 0.16919392912862344 % Correct: 0.26\n",
      "Loss: 0.1701121442444704 % Correct: 0.28\n",
      "Loss: 0.16907704098402426 % Correct: 0.26\n",
      "Loss: 0.1703992854074287 % Correct: 0.25\n",
      "Loss: 0.16873430447825816 % Correct: 0.26\n",
      "Loss: 0.1703933958386556 % Correct: 0.24\n",
      "Loss: 0.16833126496111492 % Correct: 0.26\n",
      "Loss: 0.17030728198981473 % Correct: 0.25\n",
      "Loss: 0.16809336195188096 % Correct: 0.25\n",
      "Loss: 0.17031724622483693 % Correct: 0.25\n",
      "Loss: 0.1681540792946884 % Correct: 0.26\n",
      "Loss: 0.17035972842025868 % Correct: 0.23\n",
      "Loss: 0.1685180287608136 % Correct: 0.28\n",
      "Loss: 0.17028857404122813 % Correct: 0.25\n",
      "Loss: 0.1690025065251908 % Correct: 0.28\n",
      "Loss: 0.17004942633798367 % Correct: 0.24\n",
      "Loss: 0.1694473809874073 % Correct: 0.3\n",
      "Loss: 0.16950208558505417 % Correct: 0.24\n",
      "Loss: 0.169511573111676 % Correct: 0.3\n",
      "Loss: 0.16856966063387485 % Correct: 0.24\n",
      "Loss: 0.16882023620131803 % Correct: 0.28\n",
      "Loss: 0.16769418972080868 % Correct: 0.24\n",
      "Loss: 0.16790783358984215 % Correct: 0.28\n",
      "Loss: 0.16715632607268954 % Correct: 0.24\n",
      "Loss: 0.16722633447098884 % Correct: 0.3\n",
      "Loss: 0.1669341037055728 % Correct: 0.24\n",
      "Loss: 0.16676403810765644 % Correct: 0.3\n",
      "Loss: 0.1669972815678604 % Correct: 0.23\n",
      "Loss: 0.16646801494895258 % Correct: 0.27\n",
      "Loss: 0.16741911849358002 % Correct: 0.24\n",
      "Loss: 0.16642898280029242 % Correct: 0.26\n",
      "Loss: 0.16841133166286013 % Correct: 0.25\n",
      "Loss: 0.16701310036365918 % Correct: 0.26\n",
      "Loss: 0.1701557025471872 % Correct: 0.25\n",
      "Loss: 0.1684769314296031 % Correct: 0.28\n",
      "Loss: 0.17148161387767025 % Correct: 0.25\n",
      "Loss: 0.16928848955549022 % Correct: 0.27\n",
      "Loss: 0.17014689584333945 % Correct: 0.25\n",
      "Loss: 0.16782752361641695 % Correct: 0.27\n",
      "Loss: 0.16774988702697038 % Correct: 0.28\n",
      "Loss: 0.16663711759229483 % Correct: 0.26\n",
      "Loss: 0.167071568077358 % Correct: 0.27\n",
      "Loss: 0.1674497472192821 % Correct: 0.26\n",
      "Loss: 0.16745165980141388 % Correct: 0.26\n",
      "Loss: 0.16831843137176064 % Correct: 0.29\n",
      "Loss: 0.16748345208946472 % Correct: 0.27\n",
      "Loss: 0.16804349954046993 % Correct: 0.3\n",
      "Loss: 0.16635013700679718 % Correct: 0.27\n",
      "Loss: 0.16654294868437564 % Correct: 0.3\n",
      "Loss: 0.16510535253678327 % Correct: 0.27\n",
      "Loss: 0.1651682190980657 % Correct: 0.3\n",
      "Loss: 0.16440177686767865 % Correct: 0.27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.16443116253974716 % Correct: 0.3\n",
      "Loss: 0.16408356988019396 % Correct: 0.28\n",
      "Loss: 0.16404755365219162 % Correct: 0.28\n",
      "Loss: 0.16395829213059157 % Correct: 0.28\n",
      "Loss: 0.16380441820444647 % Correct: 0.28\n",
      "Loss: 0.16390700556849233 % Correct: 0.28\n",
      "Loss: 0.16360005453676266 % Correct: 0.28\n",
      "Loss: 0.16387279630454368 % Correct: 0.26\n",
      "Loss: 0.16341041640838483 % Correct: 0.29\n",
      "Loss: 0.16384153073874808 % Correct: 0.26\n",
      "Loss: 0.1632654308657104 % Correct: 0.28\n",
      "Loss: 0.16381072393575752 % Correct: 0.26\n",
      "Loss: 0.16322716331400167 % Correct: 0.28\n",
      "Loss: 0.163746409730807 % Correct: 0.26\n",
      "Loss: 0.1633505549841071 % Correct: 0.29\n",
      "Loss: 0.1635526738863189 % Correct: 0.26\n",
      "Loss: 0.16360413288875658 % Correct: 0.29\n",
      "Loss: 0.16312599753919352 % Correct: 0.26\n",
      "Loss: 0.16382526603910508 % Correct: 0.29\n",
      "Loss: 0.16249896898547694 % Correct: 0.28\n",
      "Loss: 0.16386442568147125 % Correct: 0.27\n",
      "Loss: 0.1618597791353975 % Correct: 0.27\n",
      "Loss: 0.16375470602402392 % Correct: 0.29\n",
      "Loss: 0.16143553697797136 % Correct: 0.28\n",
      "Loss: 0.16363694531210102 % Correct: 0.3\n",
      "Loss: 0.16145120443924882 % Correct: 0.27\n",
      "Loss: 0.16359192272635398 % Correct: 0.3\n",
      "Loss: 0.1620447345112968 % Correct: 0.28\n",
      "Loss: 0.1634557681114378 % Correct: 0.3\n",
      "Loss: 0.16286971515811585 % Correct: 0.28\n",
      "Loss: 0.16279978222414743 % Correct: 0.28\n",
      "Loss: 0.16291358622409183 % Correct: 0.27\n",
      "Loss: 0.16169773618752734 % Correct: 0.28\n",
      "Loss: 0.1621982141268272 % Correct: 0.28\n",
      "Loss: 0.1608453741109104 % Correct: 0.34\n",
      "Loss: 0.16193333034956367 % Correct: 0.28\n",
      "Loss: 0.16055368253296248 % Correct: 0.32\n",
      "Loss: 0.16210105333909655 % Correct: 0.3\n",
      "Loss: 0.16012522898794107 % Correct: 0.31\n",
      "Loss: 0.16133520496351295 % Correct: 0.31\n",
      "Loss: 0.160000935251931 % Correct: 0.29\n",
      "Loss: 0.1611972185141644 % Correct: 0.3\n",
      "Loss: 0.16124696696619148 % Correct: 0.28\n",
      "Loss: 0.1629140354396886 % Correct: 0.29\n",
      "Loss: 0.16308504397462906 % Correct: 0.27\n",
      "Loss: 0.16472066902616284 % Correct: 0.26\n",
      "Loss: 0.16311435861569132 % Correct: 0.27\n",
      "Loss: 0.16394264762819674 % Correct: 0.26\n",
      "Loss: 0.16246837897216265 % Correct: 0.31\n",
      "Loss: 0.1626476777054553 % Correct: 0.26\n",
      "Loss: 0.16197993148507062 % Correct: 0.31\n",
      "Loss: 0.1618884385229228 % Correct: 0.26\n",
      "Loss: 0.16158602360261448 % Correct: 0.31\n",
      "Loss: 0.16143507937082 % Correct: 0.26\n",
      "Loss: 0.1612931718684124 % Correct: 0.31\n",
      "Loss: 0.16112260395470795 % Correct: 0.27\n",
      "Loss: 0.16107297039244386 % Correct: 0.31\n",
      "Loss: 0.1608780654197382 % Correct: 0.27\n",
      "Loss: 0.16089280910241915 % Correct: 0.31\n",
      "Loss: 0.16068492494146364 % Correct: 0.27\n",
      "Loss: 0.1607360249548365 % Correct: 0.31\n",
      "Loss: 0.16056768604794783 % Correct: 0.27\n",
      "Loss: 0.16060901866439117 % Correct: 0.31\n",
      "Loss: 0.16063128437591428 % Correct: 0.27\n",
      "Loss: 0.16056288635503 % Correct: 0.31\n",
      "Loss: 0.16109860881061017 % Correct: 0.28\n",
      "Loss: 0.16046847085036833 % Correct: 0.31\n",
      "Loss: 0.16144008277993405 % Correct: 0.27\n",
      "Loss: 0.15979872916864174 % Correct: 0.29\n",
      "Loss: 0.16050689459486855 % Correct: 0.28\n",
      "Loss: 0.15922849971625025 % Correct: 0.29\n",
      "Loss: 0.1597786549432266 % Correct: 0.27\n",
      "Loss: 0.15929538864703746 % Correct: 0.29\n",
      "Loss: 0.1596033152141915 % Correct: 0.27\n",
      "Loss: 0.15998079144776206 % Correct: 0.29\n",
      "Loss: 0.1602417049796546 % Correct: 0.27\n",
      "Loss: 0.16303359415493762 % Correct: 0.27\n",
      "Loss: 0.15979406429638707 % Correct: 0.3\n",
      "Loss: 0.16115785130596177 % Correct: 0.27\n",
      "Loss: 0.15939977467130817 % Correct: 0.28\n",
      "Loss: 0.16061648078618324 % Correct: 0.26\n",
      "Loss: 0.16065891184277117 % Correct: 0.28\n",
      "Loss: 0.1613470595084878 % Correct: 0.27\n",
      "Loss: 0.16134344335717837 % Correct: 0.28\n",
      "Loss: 0.16140194221939017 % Correct: 0.27\n",
      "Loss: 0.1602247386014772 % Correct: 0.28\n",
      "Loss: 0.1606352517995417 % Correct: 0.27\n",
      "Loss: 0.1590432475178872 % Correct: 0.28\n",
      "Loss: 0.15946126460895665 % Correct: 0.28\n",
      "Loss: 0.15863375494805268 % Correct: 0.28\n",
      "Loss: 0.15905559026824195 % Correct: 0.28\n",
      "Loss: 0.1584625719016895 % Correct: 0.28\n",
      "Loss: 0.15888598916390706 % Correct: 0.27\n",
      "Loss: 0.15822886634796746 % Correct: 0.28\n",
      "Loss: 0.1587005382343059 % Correct: 0.28\n",
      "Loss: 0.15798442890844694 % Correct: 0.28\n",
      "Loss: 0.15848568301811508 % Correct: 0.28\n",
      "Loss: 0.15780490930437602 % Correct: 0.31\n",
      "Loss: 0.15832551893028965 % Correct: 0.29\n",
      "Loss: 0.15767211396433953 % Correct: 0.31\n",
      "Loss: 0.15820357018827336 % Correct: 0.29\n",
      "Loss: 0.1575442554853697 % Correct: 0.31\n",
      "Loss: 0.15808906048840254 % Correct: 0.29\n",
      "Loss: 0.1574080113038989 % Correct: 0.31\n",
      "Loss: 0.15796900123662397 % Correct: 0.3\n",
      "Loss: 0.15727003923812582 % Correct: 0.31\n",
      "Loss: 0.15784673605675698 % Correct: 0.3\n",
      "Loss: 0.15713782446958505 % Correct: 0.31\n",
      "Loss: 0.15772584126979 % Correct: 0.31\n",
      "Loss: 0.15701276859498564 % Correct: 0.31\n",
      "Loss: 0.15760626532062166 % Correct: 0.31\n",
      "Loss: 0.15689276931836832 % Correct: 0.31\n",
      "Loss: 0.1574860822787299 % Correct: 0.31\n",
      "Loss: 0.1567757172521236 % Correct: 0.29\n",
      "Loss: 0.15736403331431773 % Correct: 0.31\n",
      "Loss: 0.15666077015122418 % Correct: 0.29\n",
      "Loss: 0.15723985015170042 % Correct: 0.31\n",
      "Loss: 0.15654810912421727 % Correct: 0.29\n",
      "Loss: 0.15711372550134686 % Correct: 0.31\n",
      "Loss: 0.1564384782735091 % Correct: 0.29\n",
      "Loss: 0.15698584509794972 % Correct: 0.31\n",
      "Loss: 0.1563331220612154 % Correct: 0.29\n",
      "Loss: 0.1568564308184516 % Correct: 0.31\n",
      "Loss: 0.1562341981106063 % Correct: 0.31\n",
      "Loss: 0.15672627200881697 % Correct: 0.31\n",
      "Loss: 0.1561457216922669 % Correct: 0.3\n",
      "Loss: 0.15659806454578387 % Correct: 0.31\n",
      "Loss: 0.156075558987905 % Correct: 0.3\n",
      "Loss: 0.15647996163733108 % Correct: 0.32\n",
      "Loss: 0.15603995445038907 % Correct: 0.3\n",
      "Loss: 0.15639520323254524 % Correct: 0.32\n",
      "Loss: 0.15607270771347564 % Correct: 0.3\n",
      "Loss: 0.15640241247872175 % Correct: 0.31\n",
      "Loss: 0.1562275263826008 % Correct: 0.28\n",
      "Loss: 0.15658902319240292 % Correct: 0.31\n",
      "Loss: 0.15646779162488064 % Correct: 0.28\n",
      "Loss: 0.15683937942482062 % Correct: 0.31\n",
      "Loss: 0.15641445964943335 % Correct: 0.28\n",
      "Loss: 0.15674996191465512 % Correct: 0.31\n",
      "Loss: 0.1561250056437303 % Correct: 0.28\n",
      "Loss: 0.15695215667063697 % Correct: 0.32\n",
      "Loss: 0.15638338411266994 % Correct: 0.29\n",
      "Loss: 0.15808937190216477 % Correct: 0.33\n",
      "Loss: 0.15741447049363294 % Correct: 0.29\n",
      "Loss: 0.1597073599380851 % Correct: 0.33\n",
      "Loss: 0.15663249698166565 % Correct: 0.3\n",
      "Loss: 0.15762117546181073 % Correct: 0.33\n",
      "Loss: 0.15514069843539893 % Correct: 0.31\n",
      "Loss: 0.1553745239410543 % Correct: 0.33\n",
      "Loss: 0.15484504232792642 % Correct: 0.32\n",
      "Loss: 0.15580395109167125 % Correct: 0.31\n",
      "Loss: 0.1551899926940124 % Correct: 0.32\n",
      "Loss: 0.15633434113555134 % Correct: 0.32\n",
      "Loss: 0.15536075165348703 % Correct: 0.31\n",
      "Loss: 0.1558202667767985 % Correct: 0.31\n",
      "Loss: 0.15469064594377172 % Correct: 0.32\n",
      "Loss: 0.15465965176845017 % Correct: 0.31\n",
      "Loss: 0.1539337422487275 % Correct: 0.32\n",
      "Loss: 0.15385676933658265 % Correct: 0.31\n",
      "Loss: 0.15347270165661817 % Correct: 0.32\n",
      "Loss: 0.1535004327291319 % Correct: 0.31\n",
      "Loss: 0.15323120404958468 % Correct: 0.32\n",
      "Loss: 0.15342239484901207 % Correct: 0.31\n",
      "Loss: 0.15311046329050573 % Correct: 0.32\n",
      "Loss: 0.15351877476869902 % Correct: 0.31\n",
      "Loss: 0.15305351062462785 % Correct: 0.32\n",
      "Loss: 0.15380005312204403 % Correct: 0.31\n",
      "Loss: 0.15309889869879548 % Correct: 0.32\n",
      "Loss: 0.15443779381918527 % Correct: 0.31\n",
      "Loss: 0.153547628893862 % Correct: 0.33\n",
      "Loss: 0.15575410044120916 % Correct: 0.33\n",
      "Loss: 0.15531865238508186 % Correct: 0.33\n",
      "Loss: 0.15714599384216732 % Correct: 0.29\n",
      "Loss: 0.15820396084484217 % Correct: 0.31\n",
      "Loss: 0.15621235265814695 % Correct: 0.28\n",
      "Loss: 0.15767458127796144 % Correct: 0.29\n",
      "Loss: 0.15565215607782026 % Correct: 0.3\n",
      "Loss: 0.15769826966264297 % Correct: 0.31\n",
      "Loss: 0.1544061816714567 % Correct: 0.3\n",
      "Loss: 0.15534612724016156 % Correct: 0.31\n",
      "Loss: 0.1540506949598881 % Correct: 0.3\n",
      "Loss: 0.15528811426673245 % Correct: 0.33\n",
      "Loss: 0.15513542941077893 % Correct: 0.31\n",
      "Loss: 0.15761469305614204 % Correct: 0.3\n",
      "Loss: 0.1618834271492895 % Correct: 0.27\n",
      "Loss: 0.17495649175506628 % Correct: 0.28\n",
      "Loss: 0.16298541484919976 % Correct: 0.29\n",
      "Loss: 0.15569297352379424 % Correct: 0.33\n",
      "Loss: 0.15337115962645825 % Correct: 0.32\n",
      "Loss: 0.15231101661175353 % Correct: 0.33\n",
      "Loss: 0.15172752036166076 % Correct: 0.34\n",
      "Loss: 0.1523429809952524 % Correct: 0.35\n",
      "Loss: 0.15219175187499448 % Correct: 0.32\n",
      "Loss: 0.15391984578686646 % Correct: 0.35\n",
      "Loss: 0.15337050342768738 % Correct: 0.28\n",
      "Loss: 0.1560409641685653 % Correct: 0.32\n",
      "Loss: 0.1538974406746384 % Correct: 0.28\n",
      "Loss: 0.1555400561749383 % Correct: 0.34\n",
      "Loss: 0.1538727965723108 % Correct: 0.28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.15488362416177157 % Correct: 0.34\n",
      "Loss: 0.15353071515353992 % Correct: 0.28\n",
      "Loss: 0.15428598990778358 % Correct: 0.34\n",
      "Loss: 0.15341018718813546 % Correct: 0.28\n",
      "Loss: 0.1538757293789456 % Correct: 0.34\n",
      "Loss: 0.1533553653078939 % Correct: 0.29\n",
      "Loss: 0.15350967665971366 % Correct: 0.34\n",
      "Loss: 0.15346682957099275 % Correct: 0.29\n",
      "Loss: 0.15308958063406017 % Correct: 0.33\n",
      "Loss: 0.15360315927511126 % Correct: 0.29\n",
      "Loss: 0.15265657340926697 % Correct: 0.33\n",
      "Loss: 0.15354287450268564 % Correct: 0.3\n",
      "Loss: 0.15234133495880187 % Correct: 0.32\n",
      "Loss: 0.15354500200473517 % Correct: 0.3\n",
      "Loss: 0.15219931125497188 % Correct: 0.32\n",
      "Loss: 0.15379956998822958 % Correct: 0.31\n",
      "Loss: 0.15234196772443329 % Correct: 0.32\n",
      "Loss: 0.15433304760156377 % Correct: 0.33\n",
      "Loss: 0.15302153932570292 % Correct: 0.32\n",
      "Loss: 0.15512873215243603 % Correct: 0.31\n",
      "Loss: 0.15482739938867657 % Correct: 0.35\n",
      "Loss: 0.1563019127611212 % Correct: 0.29\n",
      "Loss: 0.15504628752989497 % Correct: 0.35\n",
      "Loss: 0.1544290608181932 % Correct: 0.29\n",
      "Loss: 0.152986741012435 % Correct: 0.35\n",
      "Loss: 0.15239129620067765 % Correct: 0.3\n",
      "Loss: 0.15208405858393304 % Correct: 0.35\n",
      "Loss: 0.1518864255404737 % Correct: 0.31\n",
      "Loss: 0.15169218914846355 % Correct: 0.35\n",
      "Loss: 0.1518791102691805 % Correct: 0.31\n",
      "Loss: 0.15198939828486285 % Correct: 0.34\n",
      "Loss: 0.15352973942325576 % Correct: 0.31\n",
      "Loss: 0.155580912704353 % Correct: 0.32\n",
      "Loss: 0.15841822322537838 % Correct: 0.33\n",
      "Loss: 0.167672263286533 % Correct: 0.28\n",
      "Loss: 0.16572291532346362 % Correct: 0.27\n",
      "Loss: 0.15991027348551093 % Correct: 0.32\n",
      "Loss: 0.1546822344032024 % Correct: 0.28\n",
      "Loss: 0.15332312823477018 % Correct: 0.35\n",
      "Loss: 0.15239906580151147 % Correct: 0.29\n",
      "Loss: 0.15233553015133836 % Correct: 0.35\n",
      "Loss: 0.15296972136079406 % Correct: 0.29\n",
      "Loss: 0.15196826564908092 % Correct: 0.35\n",
      "Loss: 0.15247684114450866 % Correct: 0.3\n",
      "Loss: 0.15150294976253156 % Correct: 0.35\n",
      "Loss: 0.151677243794413 % Correct: 0.3\n",
      "Loss: 0.1512031524892888 % Correct: 0.35\n",
      "Loss: 0.15117165344746014 % Correct: 0.3\n",
      "Loss: 0.15125928691384782 % Correct: 0.35\n",
      "Loss: 0.15119613178085595 % Correct: 0.29\n",
      "Loss: 0.15182757198365354 % Correct: 0.35\n",
      "Loss: 0.15167502493838686 % Correct: 0.28\n",
      "Loss: 0.15279980034611612 % Correct: 0.33\n",
      "Loss: 0.15179089119859787 % Correct: 0.29\n",
      "Loss: 0.15195003481354724 % Correct: 0.35\n",
      "Loss: 0.15069130100370354 % Correct: 0.31\n",
      "Loss: 0.15055371861071437 % Correct: 0.34\n",
      "Loss: 0.1502018983157055 % Correct: 0.34\n",
      "Loss: 0.15003105150060858 % Correct: 0.34\n",
      "Loss: 0.15035454870602 % Correct: 0.35\n",
      "Loss: 0.14994762526660502 % Correct: 0.34\n",
      "Loss: 0.1506778950644748 % Correct: 0.35\n",
      "Loss: 0.1500057390821164 % Correct: 0.36\n",
      "Loss: 0.15102827395001225 % Correct: 0.33\n",
      "Loss: 0.1501244794802721 % Correct: 0.36\n",
      "Loss: 0.15099040490208995 % Correct: 0.33\n",
      "Loss: 0.15129185957774627 % Correct: 0.32\n",
      "Loss: 0.15183883310683668 % Correct: 0.35\n",
      "Loss: 0.15905826158633524 % Correct: 0.33\n",
      "Loss: 0.15036683740588708 % Correct: 0.34\n",
      "Loss: 0.15525553077428833 % Correct: 0.33\n",
      "Loss: 0.15196169191764414 % Correct: 0.34\n",
      "Loss: 0.15925244367419505 % Correct: 0.29\n",
      "Loss: 0.15296812148059166 % Correct: 0.34\n",
      "Loss: 0.15367783743137703 % Correct: 0.32\n",
      "Loss: 0.1501693663831229 % Correct: 0.34\n",
      "Loss: 0.14927918007585012 % Correct: 0.32\n",
      "Loss: 0.148932458903173 % Correct: 0.34\n",
      "Loss: 0.14805862634749892 % Correct: 0.34\n",
      "Loss: 0.14913192510457635 % Correct: 0.33\n",
      "Loss: 0.1480664882907135 % Correct: 0.33\n",
      "Loss: 0.14886553811306227 % Correct: 0.35\n",
      "Loss: 0.14804347840364387 % Correct: 0.33\n",
      "Loss: 0.1484660575698335 % Correct: 0.35\n",
      "Loss: 0.14792491725865498 % Correct: 0.33\n",
      "Loss: 0.14824147482789546 % Correct: 0.35\n",
      "Loss: 0.14785681510875476 % Correct: 0.33\n",
      "Loss: 0.1483024670227285 % Correct: 0.35\n",
      "Loss: 0.14875086712168378 % Correct: 0.34\n",
      "Loss: 0.14950174745353226 % Correct: 0.34\n",
      "Loss: 0.15046315558762036 % Correct: 0.34\n",
      "Loss: 0.15161631878327178 % Correct: 0.31\n",
      "Loss: 0.15099978497897532 % Correct: 0.34\n",
      "Loss: 0.15162036806688153 % Correct: 0.29\n",
      "Loss: 0.1518701664872311 % Correct: 0.34\n",
      "Loss: 0.1532451016850331 % Correct: 0.3\n",
      "Loss: 0.15151166194037607 % Correct: 0.33\n",
      "Loss: 0.152933407112311 % Correct: 0.31\n",
      "Loss: 0.14996064491126343 % Correct: 0.33\n",
      "Loss: 0.15032473077000325 % Correct: 0.34\n",
      "Loss: 0.148687816335934 % Correct: 0.34\n",
      "Loss: 0.14942047905539854 % Correct: 0.35\n",
      "Loss: 0.1490149884689856 % Correct: 0.33\n",
      "Loss: 0.15022818118341136 % Correct: 0.35\n",
      "Loss: 0.14859270696750662 % Correct: 0.33\n",
      "Loss: 0.14911406669590063 % Correct: 0.35\n",
      "Loss: 0.1481741222233812 % Correct: 0.34\n",
      "Loss: 0.15206970204498832 % Correct: 0.3\n",
      "Loss: 0.1482821771424804 % Correct: 0.35\n",
      "Loss: 0.14859525602567308 % Correct: 0.35\n",
      "Loss: 0.1471941283852174 % Correct: 0.36\n",
      "Loss: 0.1474550265909814 % Correct: 0.35\n",
      "Loss: 0.14712689862325243 % Correct: 0.36\n",
      "Loss: 0.1474629104347705 % Correct: 0.35\n",
      "Loss: 0.1476699296796507 % Correct: 0.36\n",
      "Loss: 0.14667678900768352 % Correct: 0.35\n",
      "Loss: 0.14692944336619737 % Correct: 0.36\n",
      "Loss: 0.1465905222006495 % Correct: 0.35\n",
      "Loss: 0.14769778607070097 % Correct: 0.35\n",
      "Loss: 0.14756182918851998 % Correct: 0.33\n",
      "Loss: 0.14833280376034036 % Correct: 0.35\n",
      "Loss: 0.14843574445564683 % Correct: 0.33\n",
      "Loss: 0.1484940199296882 % Correct: 0.35\n",
      "Loss: 0.14902611112755054 % Correct: 0.33\n",
      "Loss: 0.14724828741439197 % Correct: 0.36\n",
      "Loss: 0.14655793116257612 % Correct: 0.33\n",
      "Loss: 0.1454507537841664 % Correct: 0.36\n",
      "Loss: 0.14462595389278626 % Correct: 0.34\n",
      "Loss: 0.14498602154089935 % Correct: 0.36\n",
      "Loss: 0.14452236328982598 % Correct: 0.34\n",
      "Loss: 0.14546700652953495 % Correct: 0.35\n",
      "Loss: 0.14524384706825372 % Correct: 0.34\n",
      "Loss: 0.14686853728305396 % Correct: 0.35\n",
      "Loss: 0.14634840258472914 % Correct: 0.34\n",
      "Loss: 0.14696475522426106 % Correct: 0.36\n",
      "Loss: 0.14703018469041118 % Correct: 0.34\n",
      "Loss: 0.14699182318015752 % Correct: 0.36\n",
      "Loss: 0.14689587117130748 % Correct: 0.34\n",
      "Loss: 0.14684263471423875 % Correct: 0.35\n",
      "Loss: 0.14665158909251147 % Correct: 0.34\n",
      "Loss: 0.14664753690963012 % Correct: 0.35\n",
      "Loss: 0.14632847443553013 % Correct: 0.34\n",
      "Loss: 0.1462831560299925 % Correct: 0.35\n",
      "Loss: 0.14595840521069287 % Correct: 0.34\n",
      "Loss: 0.14639363571835723 % Correct: 0.35\n",
      "Loss: 0.14566037211524233 % Correct: 0.34\n",
      "Loss: 0.1461538981795165 % Correct: 0.36\n",
      "Loss: 0.14530995510262282 % Correct: 0.36\n",
      "Loss: 0.14588198858872076 % Correct: 0.36\n",
      "Loss: 0.14488604721809373 % Correct: 0.36\n",
      "Loss: 0.14537181618215705 % Correct: 0.36\n",
      "Loss: 0.14464441582568438 % Correct: 0.35\n",
      "Loss: 0.1454631431080326 % Correct: 0.36\n",
      "Loss: 0.14462514812303356 % Correct: 0.35\n",
      "Loss: 0.14551249531730612 % Correct: 0.36\n",
      "Loss: 0.14475112797908635 % Correct: 0.35\n",
      "Loss: 0.14573961399955987 % Correct: 0.37\n",
      "Loss: 0.1450014117470906 % Correct: 0.34\n",
      "Loss: 0.1458921476632004 % Correct: 0.35\n",
      "Loss: 0.1453444452377954 % Correct: 0.34\n",
      "Loss: 0.145984121337234 % Correct: 0.37\n",
      "Loss: 0.14553592051619463 % Correct: 0.35\n",
      "Loss: 0.14561293953843008 % Correct: 0.37\n",
      "Loss: 0.1452450531331762 % Correct: 0.35\n",
      "Loss: 0.14502971064049644 % Correct: 0.37\n",
      "Loss: 0.14468901271128679 % Correct: 0.35\n",
      "Loss: 0.14470439083510284 % Correct: 0.37\n",
      "Loss: 0.1442210185334003 % Correct: 0.35\n",
      "Loss: 0.14474928072318333 % Correct: 0.37\n",
      "Loss: 0.1438499169521279 % Correct: 0.35\n",
      "Loss: 0.14434974580986742 % Correct: 0.37\n",
      "Loss: 0.14343154489172877 % Correct: 0.35\n",
      "Loss: 0.1440014687470912 % Correct: 0.36\n",
      "Loss: 0.14307672155118364 % Correct: 0.35\n",
      "Loss: 0.14366919921823568 % Correct: 0.36\n",
      "Loss: 0.14328298847372725 % Correct: 0.35\n",
      "Loss: 0.14437241563826228 % Correct: 0.37\n",
      "Loss: 0.14438111804761805 % Correct: 0.36\n",
      "Loss: 0.14504223918590156 % Correct: 0.37\n",
      "Loss: 0.1460952844988282 % Correct: 0.34\n",
      "Loss: 0.14599244278268922 % Correct: 0.34\n",
      "Loss: 0.15131987757940793 % Correct: 0.34\n",
      "Loss: 0.15073094531704925 % Correct: 0.33\n",
      "Loss: 0.150938828175089 % Correct: 0.33\n",
      "Loss: 0.1495429384920684 % Correct: 0.34\n",
      "Loss: 0.14625238116362788 % Correct: 0.35\n",
      "Loss: 0.14541236715544398 % Correct: 0.36\n",
      "Loss: 0.14370062706055525 % Correct: 0.36\n",
      "Loss: 0.14344564206281754 % Correct: 0.36\n",
      "Loss: 0.1429513643798704 % Correct: 0.37\n",
      "Loss: 0.14432915737544508 % Correct: 0.36\n",
      "Loss: 0.14397423233538115 % Correct: 0.37\n",
      "Loss: 0.14493863722032868 % Correct: 0.36\n",
      "Loss: 0.144487339220132 % Correct: 0.36\n",
      "Loss: 0.14672205198974758 % Correct: 0.34\n",
      "Loss: 0.14484780105053927 % Correct: 0.36\n",
      "Loss: 0.14451711470838852 % Correct: 0.35\n",
      "Loss: 0.14490893949146164 % Correct: 0.35\n",
      "Loss: 0.1449835448923879 % Correct: 0.36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.1465039575322506 % Correct: 0.34\n",
      "Loss: 0.14449620381945244 % Correct: 0.35\n",
      "Loss: 0.1448399144772567 % Correct: 0.34\n",
      "Loss: 0.14345049566419 % Correct: 0.35\n",
      "Loss: 0.14372660351610314 % Correct: 0.35\n",
      "Loss: 0.14274679169862098 % Correct: 0.35\n",
      "Loss: 0.1431503588848199 % Correct: 0.35\n",
      "Loss: 0.14235837846568894 % Correct: 0.35\n",
      "Loss: 0.14289495315316542 % Correct: 0.36\n",
      "Loss: 0.14218862080825678 % Correct: 0.35\n",
      "Loss: 0.1428295572990615 % Correct: 0.37\n",
      "Loss: 0.14212380308428446 % Correct: 0.35\n",
      "Loss: 0.14294680866943016 % Correct: 0.37\n",
      "Loss: 0.14205931333723543 % Correct: 0.35\n",
      "Loss: 0.14286188279172643 % Correct: 0.36\n",
      "Loss: 0.14217977638484264 % Correct: 0.35\n",
      "Loss: 0.14292996793015705 % Correct: 0.36\n",
      "Loss: 0.14219285429642364 % Correct: 0.36\n",
      "Loss: 0.1427300964625828 % Correct: 0.36\n",
      "Loss: 0.14208710412606015 % Correct: 0.36\n",
      "Loss: 0.14293409586533767 % Correct: 0.35\n",
      "Loss: 0.14235368201266538 % Correct: 0.36\n",
      "Loss: 0.14319709226034744 % Correct: 0.35\n",
      "Loss: 0.14271506856052657 % Correct: 0.36\n",
      "Loss: 0.14351041429080372 % Correct: 0.36\n",
      "Loss: 0.1432782234486652 % Correct: 0.36\n",
      "Loss: 0.1442743241401924 % Correct: 0.33\n",
      "Loss: 0.14382877709141315 % Correct: 0.36\n",
      "Loss: 0.1445019577600002 % Correct: 0.35\n",
      "Loss: 0.14260081157110818 % Correct: 0.35\n",
      "Loss: 0.14273739165616178 % Correct: 0.36\n",
      "Loss: 0.14235076905580651 % Correct: 0.36\n",
      "Loss: 0.14488335617412448 % Correct: 0.36\n",
      "Loss: 0.14139424759460334 % Correct: 0.34\n",
      "Loss: 0.14168658454389102 % Correct: 0.37\n",
      "Loss: 0.14127315833943443 % Correct: 0.35\n",
      "Loss: 0.14198051775429182 % Correct: 0.37\n",
      "Loss: 0.14235420608155402 % Correct: 0.34\n",
      "Loss: 0.1442683865746681 % Correct: 0.35\n",
      "Loss: 0.1459806002090732 % Correct: 0.33\n",
      "Loss: 0.1464790364751442 % Correct: 0.37\n",
      "Loss: 0.1445625985693469 % Correct: 0.34\n",
      "Loss: 0.14242353275693806 % Correct: 0.37\n",
      "Loss: 0.1415415530575979 % Correct: 0.36\n",
      "Loss: 0.14153761859075786 % Correct: 0.37\n",
      "Loss: 0.1410114264469825 % Correct: 0.36\n",
      "Loss: 0.1417270467811488 % Correct: 0.37\n",
      "Loss: 0.1410802095460389 % Correct: 0.35\n",
      "Loss: 0.1420221848666124 % Correct: 0.36\n",
      "Loss: 0.14071342476779966 % Correct: 0.35\n",
      "Loss: 0.14150536782109494 % Correct: 0.36\n",
      "Loss: 0.14050192180602636 % Correct: 0.35\n",
      "Loss: 0.1412618686526018 % Correct: 0.37\n",
      "Loss: 0.14074296172540568 % Correct: 0.35\n",
      "Loss: 0.14139051615984702 % Correct: 0.37\n",
      "Loss: 0.1418823972608137 % Correct: 0.35\n",
      "Loss: 0.14320435595186018 % Correct: 0.37\n",
      "Loss: 0.14263870913454552 % Correct: 0.35\n",
      "Loss: 0.14169562337025393 % Correct: 0.37\n",
      "Loss: 0.14168482316982606 % Correct: 0.35\n",
      "Loss: 0.1421184889102944 % Correct: 0.37\n",
      "Loss: 0.14100095169661925 % Correct: 0.34\n",
      "Loss: 0.14014484326376475 % Correct: 0.37\n",
      "Loss: 0.14059641083639723 % Correct: 0.34\n",
      "Loss: 0.14089587841443554 % Correct: 0.37\n",
      "Loss: 0.14092693598205314 % Correct: 0.34\n",
      "Loss: 0.14005424162271735 % Correct: 0.37\n",
      "Loss: 0.14053685465163376 % Correct: 0.36\n",
      "Loss: 0.14401239958738085 % Correct: 0.36\n",
      "Loss: 0.14768135157187945 % Correct: 0.33\n",
      "Loss: 0.16594412848222453 % Correct: 0.29\n",
      "Loss: 0.1522036308104584 % Correct: 0.34\n",
      "Loss: 0.1511441027470445 % Correct: 0.33\n",
      "Loss: 0.15469480567301955 % Correct: 0.36\n",
      "Loss: 0.15260784823057422 % Correct: 0.33\n",
      "Loss: 0.14746779149698516 % Correct: 0.34\n",
      "Loss: 0.14835971240540724 % Correct: 0.36\n",
      "Loss: 0.1433033602073332 % Correct: 0.35\n",
      "Loss: 0.14269003470918445 % Correct: 0.36\n",
      "Loss: 0.14219735439459213 % Correct: 0.36\n",
      "Loss: 0.14342429550999908 % Correct: 0.36\n",
      "Loss: 0.14683962697429506 % Correct: 0.35\n",
      "Loss: 0.14680862988179383 % Correct: 0.35\n",
      "Loss: 0.14779827251437913 % Correct: 0.35\n",
      "Loss: 0.14948661555460482 % Correct: 0.35\n",
      "Loss: 0.1539282442197544 % Correct: 0.32\n",
      "Loss: 0.14663696486662486 % Correct: 0.34\n",
      "Loss: 0.14921082247069953 % Correct: 0.34\n",
      "Loss: 0.14524717889667843 % Correct: 0.35\n",
      "Loss: 0.14398677262156223 % Correct: 0.34\n",
      "Loss: 0.1401938242631398 % Correct: 0.36\n",
      "Loss: 0.13958181156703925 % Correct: 0.36\n",
      "Loss: 0.1388568788033182 % Correct: 0.36\n",
      "Loss: 0.13865998333294333 % Correct: 0.35\n",
      "Loss: 0.1383805099297953 % Correct: 0.36\n",
      "Loss: 0.13835718947911346 % Correct: 0.35\n",
      "Loss: 0.138234657172791 % Correct: 0.37\n",
      "Loss: 0.13840615111423163 % Correct: 0.35\n",
      "Loss: 0.13838179908625564 % Correct: 0.37\n",
      "Loss: 0.1388567651015807 % Correct: 0.35\n",
      "Loss: 0.13881427018672404 % Correct: 0.37\n",
      "Loss: 0.13974836990745843 % Correct: 0.35\n",
      "Loss: 0.13953079034770782 % Correct: 0.36\n",
      "Loss: 0.1409683309388801 % Correct: 0.36\n",
      "Loss: 0.1401246899789234 % Correct: 0.35\n",
      "Loss: 0.1405722954385031 % Correct: 0.36\n",
      "Loss: 0.14000234497188896 % Correct: 0.35\n",
      "Loss: 0.14016373027446039 % Correct: 0.36\n",
      "Loss: 0.13962070793581488 % Correct: 0.35\n",
      "Loss: 0.13931918100534443 % Correct: 0.37\n",
      "Loss: 0.13905956821302207 % Correct: 0.35\n",
      "Loss: 0.1393743701284376 % Correct: 0.37\n",
      "Loss: 0.13897456132728592 % Correct: 0.35\n",
      "Loss: 0.13899386832273036 % Correct: 0.37\n",
      "Loss: 0.13875747394148166 % Correct: 0.35\n",
      "Loss: 0.13926687983271344 % Correct: 0.37\n",
      "Loss: 0.13890230246920282 % Correct: 0.35\n",
      "Loss: 0.13919840561480792 % Correct: 0.37\n",
      "Loss: 0.13890870667800806 % Correct: 0.35\n",
      "Loss: 0.13968796478692608 % Correct: 0.36\n",
      "Loss: 0.13921286468236482 % Correct: 0.35\n",
      "Loss: 0.13973662655072072 % Correct: 0.36\n",
      "Loss: 0.13917089972959987 % Correct: 0.35\n",
      "Loss: 0.1401317314755819 % Correct: 0.36\n",
      "Loss: 0.13953994634583253 % Correct: 0.35\n",
      "Loss: 0.14014406762243375 % Correct: 0.36\n",
      "Loss: 0.13934903182075903 % Correct: 0.35\n",
      "Loss: 0.1403635898810422 % Correct: 0.36\n",
      "Loss: 0.1396272326709907 % Correct: 0.35\n",
      "Loss: 0.14011739423145533 % Correct: 0.36\n",
      "Loss: 0.13924920943292302 % Correct: 0.35\n",
      "Loss: 0.1402941234122134 % Correct: 0.36\n",
      "Loss: 0.13932870993070431 % Correct: 0.35\n",
      "Loss: 0.13975517805013532 % Correct: 0.36\n",
      "Loss: 0.1389415390513375 % Correct: 0.35\n",
      "Loss: 0.14030413834310612 % Correct: 0.36\n",
      "Loss: 0.13913878483365666 % Correct: 0.36\n",
      "Loss: 0.13979856332047289 % Correct: 0.36\n",
      "Loss: 0.1391112924885635 % Correct: 0.36\n",
      "Loss: 0.14145836403495146 % Correct: 0.36\n",
      "Loss: 0.14070485329619353 % Correct: 0.36\n",
      "Loss: 0.14031127028539944 % Correct: 0.36\n",
      "Loss: 0.1399335611936996 % Correct: 0.36\n",
      "Loss: 0.14051008660641479 % Correct: 0.33\n",
      "Loss: 0.1412981501295649 % Correct: 0.36\n",
      "Loss: 0.1396789445915947 % Correct: 0.35\n",
      "Loss: 0.1403087908876795 % Correct: 0.36\n",
      "Loss: 0.14083436454488846 % Correct: 0.36\n",
      "Loss: 0.140704217366488 % Correct: 0.36\n",
      "Loss: 0.1392847271627139 % Correct: 0.37\n",
      "Loss: 0.13992709534097025 % Correct: 0.36\n",
      "Loss: 0.140869587541067 % Correct: 0.35\n",
      "Loss: 0.1404939324276213 % Correct: 0.36\n",
      "Loss: 0.1392290750005891 % Correct: 0.37\n",
      "Loss: 0.1389359785440181 % Correct: 0.36\n",
      "Loss: 0.1393669266523826 % Correct: 0.35\n",
      "Loss: 0.138953713611155 % Correct: 0.36\n",
      "Loss: 0.138326113987067 % Correct: 0.37\n",
      "Loss: 0.13849715641364332 % Correct: 0.36\n",
      "Loss: 0.13899343054966015 % Correct: 0.37\n",
      "Loss: 0.13893156032683424 % Correct: 0.36\n",
      "Loss: 0.13899413681824654 % Correct: 0.37\n",
      "Loss: 0.13966545957879836 % Correct: 0.35\n",
      "Loss: 0.14084227608287156 % Correct: 0.37\n",
      "Loss: 0.1407846577527305 % Correct: 0.35\n",
      "Loss: 0.14050571521365646 % Correct: 0.36\n",
      "Loss: 0.140765962315266 % Correct: 0.35\n",
      "Loss: 0.14183425555002876 % Correct: 0.37\n",
      "Loss: 0.14158102044118753 % Correct: 0.34\n",
      "Loss: 0.1419766199546148 % Correct: 0.37\n",
      "Loss: 0.14020540287122543 % Correct: 0.35\n",
      "Loss: 0.1413078485755453 % Correct: 0.37\n",
      "Loss: 0.1382294057734084 % Correct: 0.35\n",
      "Loss: 0.13829446740102197 % Correct: 0.37\n",
      "Loss: 0.13753849434942014 % Correct: 0.36\n",
      "Loss: 0.13806582081682067 % Correct: 0.37\n",
      "Loss: 0.13747715127840088 % Correct: 0.36\n",
      "Loss: 0.13807397751666056 % Correct: 0.37\n",
      "Loss: 0.1374746722492331 % Correct: 0.36\n",
      "Loss: 0.1381611040346434 % Correct: 0.37\n",
      "Loss: 0.13742845632899311 % Correct: 0.35\n",
      "Loss: 0.1381127440782643 % Correct: 0.37\n",
      "Loss: 0.13733531422418996 % Correct: 0.35\n",
      "Loss: 0.13802288089406972 % Correct: 0.37\n",
      "Loss: 0.1372106292812133 % Correct: 0.35\n",
      "Loss: 0.13786791998830944 % Correct: 0.37\n",
      "Loss: 0.13708816299922905 % Correct: 0.36\n",
      "Loss: 0.1377644997415322 % Correct: 0.37\n",
      "Loss: 0.1370096129117913 % Correct: 0.36\n",
      "Loss: 0.13770187107681428 % Correct: 0.37\n",
      "Loss: 0.13697490626078643 % Correct: 0.36\n",
      "Loss: 0.1376734708356701 % Correct: 0.37\n",
      "Loss: 0.13692740409544574 % Correct: 0.36\n",
      "Loss: 0.1376512199100976 % Correct: 0.37\n",
      "Loss: 0.1369203292031071 % Correct: 0.36\n",
      "Loss: 0.1378791708617135 % Correct: 0.37\n",
      "Loss: 0.13742547701216354 % Correct: 0.35\n",
      "Loss: 0.13920635070812667 % Correct: 0.37\n",
      "Loss: 0.1403624206202167 % Correct: 0.35\n",
      "Loss: 0.14109410171850928 % Correct: 0.37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.14359010998925026 % Correct: 0.36\n",
      "Loss: 0.14075991650006264 % Correct: 0.36\n",
      "Loss: 0.14164996569290017 % Correct: 0.36\n",
      "Loss: 0.13828673369417627 % Correct: 0.37\n",
      "Loss: 0.13800479649724356 % Correct: 0.36\n"
     ]
    }
   ],
   "source": [
    "embed = Embedding(vocab_size = len(vocab), dim = 16)\n",
    "model = RNNCell(n_inputs = 16, n_hidden = 16, n_output = len(vocab))\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "params = model.get_parameters() + embed.get_parameters()\n",
    "\n",
    "optim = SGD(parameters = params, alpha = 0.05)\n",
    "\n",
    "for j in range(1000):\n",
    "    batch_size = 100\n",
    "    total_loss = 0\n",
    "    \n",
    "    hidden = model.init_hidden(batch_size = batch_size)\n",
    "    \n",
    "    for t in range(5):\n",
    "        input = Tensor(data[0:batch_size, t], autograd = True)\n",
    "        rnn_input = embed.forward(input = input)\n",
    "        output, hidden = model.forward(input = rnn_input, hidden = hidden)\n",
    "        \n",
    "    target = Tensor(data[0:batch_size, t+1], autograd = True)\n",
    "    loss = criterion.forward(output, target)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    total_loss += loss.data\n",
    "    \n",
    "    p_correct = (target.data == np.argmax(output.data,axis=1)).mean()        \n",
    "    print_loss = total_loss / (len(data)/batch_size)        \n",
    "    print(\"Loss:\",print_loss,\"% Correct:\",p_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: - \n",
      "Pred: office.\n",
      "Context: - mary \n",
      "Pred: office.\n",
      "Context: - mary moved \n",
      "Pred: office.\n",
      "Context: - mary moved to \n",
      "Pred: office.\n",
      "Context: - mary moved to the \n",
      "Pred: office.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "hidden = model.init_hidden(batch_size = batch_size)\n",
    "for t in range(5):\n",
    "    input = Tensor(data[0:batch_size, t], autograd = True)\n",
    "    rnn_input = embed.forward(input = input)\n",
    "    output, hidden = model.forward(input = rnn_input, hidden = hidden)\n",
    "    \n",
    "target = Tensor(data[0:batch_size, t+1], autograd = True)\n",
    "loss = criterion.forward(output, target)\n",
    "\n",
    "ctx = \"\" \n",
    "for idx in data[0:batch_size][0][0:-1]:    \n",
    "    ctx += vocab[idx] + \" \" \n",
    "    print(\"Context:\",ctx) \n",
    "    print(\"Pred:\", vocab[output.data.argmax()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
